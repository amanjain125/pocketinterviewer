import { useState, useEffect } from 'react';
import { useAppState } from '@/hooks/useAppState';
import { useSpeechRecognition } from '@/hooks/useSpeechRecognition';
import { Button } from '@/components/ui/button';
import {
  Mic,
  MicOff,
  Clock,
  MessageSquare,
  ArrowRight,
  Loader2,
  Volume2,
  X,
  VolumeX
} from 'lucide-react';

// Define mock questions as fallback
const mockQuestions: Record<string, string[]> = {
  behavioral: [
    "Tell me about a time when you faced a difficult challenge at work or school. How did you handle it?",
    "Describe a situation where you had to work with a difficult team member. What did you do?",
    "Can you share an example of when you had to meet a tight deadline? How did you manage it?",
  ],
  technical: [
    "Explain the difference between a stack and a queue. When would you use each?",
    "What is the time complexity of binary search? Can you explain how it works?",
    "Describe how a hash table works and what makes it efficient for lookups.",
  ],
  rapid_fire: [
    "What is your greatest strength?",
    "Where do you see yourself in 5 years?",
    "Why should we hire you?",
    "What motivates you?",
  ],
  situational: [
    "Imagine you're given a project with an impossible deadline. What do you do?",
    "A team member is not pulling their weight on a critical project. How do you handle it?",
    "You disagree with your manager's approach to a problem. What's your next step?",
  ],
  hr_basics: [
    "What are your salary expectations?",
    "Are you comfortable with relocation if required?",
    "Tell me about a time you had to adapt to a major change at work.",
  ],
};

// Questions will be generated by Ollama based on interview type
const generateQuestionsWithOllama = async (interviewType: string, difficulty: string, numQuestions: number = 5) => {
  try {
    const prompt = `
      Generate ${numQuestions} ${difficulty} ${interviewType} interview questions.
      Return only the questions as a JSON array of strings.
      Make sure the questions are relevant to ${interviewType} interviews and appropriate for ${difficulty} level.
    `;

    const response = await fetch('http://localhost:11434/api/generate', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model: 'llama3.2:3b',
        prompt: prompt,
        stream: false
      })
    });

    const data = await response.json();
    
    // Extract questions from response - this may vary depending on Ollama response format
    // Attempt to parse as JSON array of questions
    try {
      const parsed = JSON.parse(data.response);
      if (Array.isArray(parsed)) {
        return parsed;
      }
    } catch (e) {
      // If direct parsing fails, try to extract questions from text
      const questionLines = data.response.split('\n').filter((line: string) =>
        line.trim() &&
        (line.includes('?') || line.startsWith('1.') || line.startsWith('2.') ||
         line.startsWith('3.') || line.startsWith('4.') || line.startsWith('5.'))
      );

      return questionLines.map((q: string) => q.replace(/^\d+\.\s*/, '').replace(/^\d+\)\s*/, '').trim()).filter((q: string) => q);
    }
    
    return [];
  } catch (error) {
    console.error('Error generating questions with Ollama:', error);
    // Fallback to mock questions if Ollama fails
    return mockQuestions[interviewType as keyof typeof mockQuestions] || mockQuestions.behavioral;
  }
};

// Define skill types
type SkillType = 'clarity' | 'structure' | 'confidence' | 'technicalDepth' | 'relevance';

// Define skill labels
const skillLabels: Record<SkillType, string> = {
  clarity: 'Clarity',
  structure: 'Structure',
  confidence: 'Confidence',
  technicalDepth: 'Technical Depth',
  relevance: 'Relevance'
};

// Define skill hints
const skillHints: Record<SkillType, string> = {
  clarity: 'Speak clearly and avoid filler words',
  structure: 'Organize your thoughts logically',
  confidence: 'Maintain steady pace and tone',
  technicalDepth: 'Include specific technical details',
  relevance: 'Stay focused on the question'
};

export function InterviewPage() {
  const { interviewConfig, endInterview, navigateTo } = useAppState();
  const [currentQuestionIndex, setCurrentQuestionIndex] = useState(0);
  const [timeLeft, setTimeLeft] = useState(300); // 5 minutes default
  const [isProcessing, setIsProcessing] = useState(false);
  const [activeSkill, setActiveSkill] = useState<SkillType>('clarity'); // Current active skill
  const [skillValue, setSkillValue] = useState<number>(50); // Skill value for the gauge (0-100)
  const [skillColor, setSkillColor] = useState<string>('#818cf8'); // Color based on skill level
  const [lastFeedback, setLastFeedback] = useState<string>(''); // Post-answer feedback
  const [questions, setQuestions] = useState<string[]>([]);
  const [currentQuestion, setCurrentQuestion] = useState<string>('');
  
  const {
    isListening,
    transcript,
    finalTranscript,
    isSupported,
    startListening,
    stopListening,
    resetTranscript
  } = useSpeechRecognition();

  // Load questions when interview starts
  useEffect(() => {
    const loadQuestions = async () => {
      if (interviewConfig?.type && interviewConfig?.difficulty) {
        const generatedQuestions = await generateQuestionsWithOllama(
          interviewConfig.type,
          interviewConfig.difficulty
        );
        setQuestions(generatedQuestions);
        
        if (generatedQuestions.length > 0) {
          setCurrentQuestion(generatedQuestions[0]);
        }
      }
    };

    loadQuestions();
  }, [interviewConfig]);

  // Update current question when index changes
  useEffect(() => {
    if (questions.length > 0 && currentQuestionIndex < questions.length) {
      setCurrentQuestion(questions[currentQuestionIndex]);
    }
  }, [currentQuestionIndex, questions]);

  // Check browser support on mount
  useEffect(() => {
    if (!isSupported) {
      console.warn('Speech recognition not supported in this browser');
    }
  }, [isSupported]);

  // Timer with mode-specific behavior
  useEffect(() => {
    if (timeLeft <= 0) {
      handleEndInterview();
      return;
    }

    const timer = setInterval(() => {
      setTimeLeft(prev => {
        let newTime = prev - 1;
        
        // In stress mode, reduce time faster to increase pressure
        if (interviewConfig?.mode === 'stress' && newTime > 0) {
          // Reduce time by an additional second every 20 seconds to increase pressure
          if (prev % 20 === 0) {
            newTime = Math.max(0, newTime - 1); // Reduce by additional second
            setLastFeedback(current => current + " âš¡ Time pressure increasing! Time reduced!");
          }
        }
        
        return newTime;
      });
    }, 1000);

    return () => clearInterval(timer);
  }, [timeLeft, interviewConfig?.mode]);

  // Real-time skill analysis with mode-specific effects
  useEffect(() => {
    if (isListening && transcript) {
      // Analyze transcript with Ollama in real-time
      const analyzeTranscript = async () => {
        try {
          let prompt = `
            Analyze the following speech sample for the skill "${activeSkill}" on a scale of 0-100.
            Speech: "${transcript}"

            Evaluate based on:
            - ${activeSkill === 'clarity' ? 'How clear and understandable the speech is' : ''}
            - ${activeSkill === 'structure' ? 'How well organized and logical the response is' : ''}
            - ${activeSkill === 'confidence' ? 'How confident and assured the speaker sounds' : ''}
            - ${activeSkill === 'technicalDepth' ? 'How technically knowledgeable the response is' : ''}
            - ${activeSkill === 'relevance' ? 'How relevant and on-topic the response is' : ''}

            Respond with only a number between 0 and 100 representing the score.
          `;

          // Add mode-specific instructions
          if (interviewConfig?.mode === 'stress') {
            prompt += "\nNote: This is in stress mode with time pressure. Consider how well the speaker handles pressure and maintains composure.";
          } else if (interviewConfig?.mode === 'distraction') {
            prompt += "\nNote: This is in distraction mode. Consider how well the speaker maintains focus and coherence despite potential distractions.";
          }

          const response = await fetch('http://localhost:11434/api/generate', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({
              model: 'llama3.2:3b',
              prompt: prompt,
              stream: false
            })
          });

          const data = await response.json();

          // Extract score from response
          const scoreMatch = data.response.match(/\d+/);
          let score = scoreMatch ? parseInt(scoreMatch[0]) : 50;

          // Apply mode-specific adjustments
          if (interviewConfig?.mode === 'stress') {
            // In stress mode, scores are more affected by pressure
            score = Math.max(0, score - 12); // Apply stress penalty
          } else if (interviewConfig?.mode === 'distraction') {
            // In distraction mode, scores are affected by focus level
            score = Math.max(0, score - 10); // Apply distraction penalty
          }

          // Ensure value stays between 0 and 100
          const value = Math.max(0, Math.min(100, score));

          // Update skill value
          setSkillValue(value);

          // Update color based on value
          if (value >= 70) {
            setSkillColor('#10b981'); // Green
          } else if (value >= 40) {
            setSkillColor('#f59e0b'); // Yellow
          } else {
            setSkillColor('#ef4444'); // Red
          }
        } catch (error) {
          console.error('Error analyzing transcript with Ollama:', error);
          // Fallback to basic analysis if Ollama fails
          const words = transcript.split(' ');
          const wordCount = words.length;
          const fillerWords = ['um', 'uh', 'like', 'so', 'well', 'actually', 'basically', 'right', 'kind of', 'sort of'];
          const fillerCount = words.filter((word: string) => fillerWords.includes(word.toLowerCase())).length;
          const fillerRatio = wordCount > 0 ? fillerCount / wordCount : 0;

          // Calculate skill value based on basic analysis
          let value = 50; // Base value
          if (fillerRatio > 0.1) {
            value -= fillerRatio * 100; // Penalize for too many fillers
          } else {
            value += (0.1 - fillerRatio) * 50; // Reward for low fillers
          }

          // Apply mode-specific adjustments
          if (interviewConfig?.mode === 'stress') {
            // In stress mode, apply additional penalty
            value = Math.max(0, value - 15); // More significant penalty in stress mode
          } else if (interviewConfig?.mode === 'distraction') {
            // In distraction mode, apply focus penalty
            value = Math.max(0, value - 12); // Penalty for distraction impact
          }

          // Ensure value stays between 0 and 100
          value = Math.max(0, Math.min(100, value));

          // Update skill value
          setSkillValue(value);

          // Update color based on value
          if (value >= 70) {
            setSkillColor('#10b981'); // Green
          } else if (value >= 40) {
            setSkillColor('#f59e0b'); // Yellow
          } else {
            setSkillColor('#ef4444'); // Red
          }
        }
      };

      // Analyze transcript more frequently in special modes
      const interval = setInterval(
        analyzeTranscript, 
        (interviewConfig?.mode === 'stress' || interviewConfig?.mode === 'distraction') ? 800 : 1000 // Every 800ms in special modes, 1000ms in normal mode
      );

      return () => clearInterval(interval);
    }
  }, [isListening, transcript, activeSkill, interviewConfig?.mode]);

  // Distraction mode effect with multiple background noise types
  useEffect(() => {
    let noiseSource: AudioBufferSourceNode | null = null;
    let audioContext: AudioContext | null = null;
    let noiseInterval: ReturnType<typeof setInterval> | null = null;
    let noiseTypeInterval: ReturnType<typeof setInterval> | null = null;
    
    if (interviewConfig?.mode === 'distraction' && isListening) {
      // Array of different noise types
      const noiseTypes = [
        'white',    // White noise - constant static
        'brown',    // Brown noise - deeper, rumbling sound
        'pink',     // Pink noise - balanced frequency spectrum
        'coffee',   // Coffee shop ambiance
        'traffic',  // Traffic noise
        'office'    // Office chatter
      ];
      
      let currentNoiseType = 'white'; // Start with white noise
      
      // Function to create different types of noise
      const createNoiseBuffer = (noiseType: string) => {
        try {
          if (!audioContext) {
            audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
          }
          
          const bufferSize = audioContext.sampleRate * 10; // 10 seconds of noise
          const buffer = audioContext.createBuffer(1, bufferSize, audioContext.sampleRate);
          const data = buffer.getChannelData(0);

          switch (noiseType) {
            case 'white':
              // White noise: completely random values
              for (let i = 0; i < bufferSize; i++) {
                data[i] = Math.random() * 2 - 1; // Random value between -1 and 1
              }
              break;
              
            case 'pink':
              // Pink noise: more natural, balanced sound
              let b0 = 0, b1 = 0, b2 = 0, b3 = 0, b4 = 0, b5 = 0, b6 = 0;
              for (let i = 0; i < bufferSize; i++) {
                const white = Math.random() * 2 - 1;
                b0 = 0.99886 * b0 + white * 0.0555179;
                b1 = 0.99332 * b1 + white * 0.0750759;
                b2 = 0.96900 * b2 + white * 0.1538520;
                b3 = 0.86650 * b3 + white * 0.3104856;
                b4 = 0.55000 * b4 + white * 0.5329522;
                b5 = -0.7616 * b5 - white * 0.0168980;
                data[i] = b0 + b1 + b2 + b3 + b4 + b5 + b6 + white * 0.5362;
                data[i] *= 0.11; // Scale to prevent clipping
              }
              break;
              
            case 'brown':
              // Brown noise: deeper, rumbling sound
              let lastValue = 0;
              for (let i = 0; i < bufferSize; i++) {
                const brown = (Math.random() * 2 - 1) * 0.1;
                lastValue = (lastValue + brown) * 0.5;
                data[i] = lastValue;
              }
              break;
              
            case 'coffee':
              // Coffee shop ambiance: mix of conversations and background sounds
              for (let i = 0; i < bufferSize; i++) {
                // Mix of different frequencies to simulate coffee shop
                const t = i / audioContext.sampleRate;
                const coffeeAmbiance = (
                  Math.sin(2 * Math.PI * 100 * t) * 0.1 + // Low hum
                  Math.sin(2 * Math.PI * 200 * t) * 0.05 + // Mid frequencies
                  Math.random() * 0.05 // Some randomness
                );
                data[i] = coffeeAmbiance;
              }
              break;
              
            case 'traffic':
              // Traffic noise: road noise simulation
              for (let i = 0; i < bufferSize; i++) {
                const t = i / audioContext.sampleRate;
                // Simulate traffic with low-frequency rumble and occasional honks
                const traffic = (
                  Math.sin(2 * Math.PI * 60 * t) * 0.2 + // Engine rumble
                  Math.sin(2 * Math.PI * 120 * t) * 0.1 + // Road noise
                  (Math.random() > 0.99 ? Math.sin(2 * Math.PI * 800 * t) * 0.3 : 0) // Occasional honk
                );
                data[i] = traffic;
              }
              break;
              
            case 'office':
              // Office chatter: low-level conversations
              for (let i = 0; i < bufferSize; i++) {
                const t = i / audioContext.sampleRate;
                // Simulate office chatter with varying volumes
                const officeChatter = (
                  Math.sin(2 * Math.PI * 200 * t) * 0.05 + // Low murmur
                  Math.sin(2 * Math.PI * 400 * t) * 0.03 + // Conversations
                  (Math.random() > 0.995 ? Math.sin(2 * Math.PI * 1000 * t) * 0.1 : 0) // Occasional phone ring
                );
                data[i] = officeChatter;
              }
              break;
              
            default:
              // Default to white noise
              for (let i = 0; i < bufferSize; i++) {
                data[i] = Math.random() * 2 - 1;
              }
          }

          return buffer;
        } catch (e) {
          console.error("Error creating noise buffer:", e);
          // Fallback to white noise
          if (!audioContext) {
            audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
          }
          const fallbackBuffer = audioContext.createBuffer(1, audioContext.sampleRate, audioContext.sampleRate);
          const fallbackData = fallbackBuffer.getChannelData(0);
          for (let i = 0; i < fallbackBuffer.length; i++) {
            fallbackData[i] = Math.random() * 2 - 1;
          }
          return fallbackBuffer;
        }
      };
      
      // Function to start playing a specific noise type
      const startNoise = (noiseType: string) => {
        if (noiseSource) {
          noiseSource.stop();
          noiseSource.disconnect();
        }
        
        const buffer = createNoiseBuffer(noiseType);
        noiseSource = audioContext!.createBufferSource();
        noiseSource.buffer = buffer;
        noiseSource.loop = true;
        
        // Connect to output with low volume
        noiseSource.connect(audioContext!.destination);
        noiseSource.start();
      };
      
      // Start with initial noise
      if (!audioContext) {
        audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
      }
      startNoise(currentNoiseType);
      
      // Change noise type periodically
      noiseTypeInterval = setInterval(() => {
        const randomNoiseType = noiseTypes[Math.floor(Math.random() * noiseTypes.length)];
        if (randomNoiseType !== currentNoiseType) {
          currentNoiseType = randomNoiseType;
          startNoise(currentNoiseType);
          console.log(`Switched to ${currentNoiseType} noise`);
        }
      }, 30000); // Change noise type every 30 seconds
      
      // Simulate periodic louder distractions
      noiseInterval = setInterval(() => {
        // Higher chance of distraction (60% every 8 seconds)
        if (Math.random() > 0.4) {
          // Different distraction messages based on noise type
          const distractionMessages = [
            "ðŸš¨ URGENT ALERT! Refocus on the question.",
            "ðŸš¨ BACKGROUND NOISE INCREASED! Stay focused.",
            "ðŸš¨ DISTRACTION! Concentrate on your answer.",
            "ðŸš¨ NOISE INTERRUPTION! Maintain your composure.",
            "ðŸš¨ ENVIRONMENTAL DISTRACTION! Focus on the question."
          ];
          
          const randomMessage = distractionMessages[Math.floor(Math.random() * distractionMessages.length)];
          setLastFeedback(randomMessage);
          
          // Add visual distraction effect
          document.body.classList.add('distraction-active');
          
          // Play a louder distraction sound based on current noise type
          let distractionSound;
          switch (currentNoiseType) {
            case 'coffee':
              distractionSound = 'data:audio/wav;base64,UklGRigAAABXQVZFZm10IBAAAAABAAEARKwAAIhYAQACABAAZGF0YQQAAAAAAAAAAAA='; // Coffee shop sound
              break;
            case 'traffic':
              distractionSound = 'data:audio/wav;base64,UklGRigAAABXQVZFZm10IBAAAAABAAEARKwAAIhYAQACABAAZGF0YQQAAAAAAAAAAAA='; // Traffic sound
              break;
            case 'office':
              distractionSound = 'data:audio/wav;base64,UklGRigAAABXQVZFZm10IBAAAAABAAEARKwAAIhYAQACABAAZGF0YQQAAAAAAAAAAAA='; // Office sound
              break;
            default:
              distractionSound = 'data:audio/wav;base64,UklGRigAAABXQVZFZm10IBAAAAABAAEARKwAAIhYAQACABAAZGF0YQQAAAAAAAAAAAA='; // General alert
          }
          
          const distractionAudio = new Audio(distractionSound);
          distractionAudio.volume = 0.4; // Louder than background noise
          distractionAudio.play().catch(e => console.log("Distraction audio play failed:", e));
          
          setTimeout(() => {
            document.body.classList.remove('distraction-active');
          }, 3000); // Remove distraction effect after 3 seconds
        }
      }, 8000); // Every 8 seconds
    }

    return () => {
      if (noiseInterval) clearInterval(noiseInterval);
      if (noiseTypeInterval) clearInterval(noiseTypeInterval);
      if (noiseSource) {
        noiseSource.stop();
        noiseSource.disconnect();
        noiseSource = null;
      }
      if (audioContext) {
        audioContext.close();
        audioContext = null;
      }
      document.body.classList.remove('distraction-active');
    };
  }, [interviewConfig?.mode, isListening]);

  const toggleListening = () => {
    if (isListening) {
      stopListening();
      // Provide post-answer feedback
      setLastFeedback(`Your ${skillLabels[activeSkill]} was ${skillValue >= 70 ? 'strong' : skillValue >= 40 ? 'needs improvement' : 'weak'}. ${skillHints[activeSkill]}`);
    } else {
      startListening();
      // Reset feedback when starting to speak
      setLastFeedback('');
    }
  };

  const handleNextQuestion = async () => {
    // Evaluate the current answer with Ollama before moving to next question
    if (transcript) {
      try {
        let evaluationPrompt = `
          Evaluate this answer to the question: "${currentQuestion}"
          Answer: "${transcript}"
          
          Provide brief feedback focusing on the current skill: ${activeSkill}.
          Be specific about what was good and what could be improved.
        `;

        // Add mode-specific instructions
        if (interviewConfig?.mode === 'stress') {
          evaluationPrompt += "\nNote: This was answered under time pressure. Consider how well the candidate handled the stress.";
        } else if (interviewConfig?.mode === 'distraction') {
          evaluationPrompt += "\nNote: This was answered while dealing with distractions. Consider how well the candidate maintained focus.";
        }

        const response = await fetch('http://localhost:11434/api/generate', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({
            model: 'llama3.2:3b',
            prompt: evaluationPrompt,
            stream: false
          })
        });

        const data = await response.json();
        setLastFeedback(data.response.substring(0, 200) + '...'); // Limit feedback length
      } catch (error) {
        console.error('Error getting feedback for current answer:', error);
        setLastFeedback('Good effort! Try to be more specific in your next answer.');
      }
    }

    if (currentQuestionIndex < questions.length - 1) {
      // Move to next question after a brief pause to show feedback
      setTimeout(() => {
        resetTranscript();
        setCurrentQuestionIndex(prev => prev + 1);
        
        // Randomly select next skill to focus on
        const skills: SkillType[] = ['clarity', 'structure', 'confidence', 'technicalDepth', 'relevance'];
        const randomSkill = skills[Math.floor(Math.random() * skills.length)];
        setActiveSkill(randomSkill);
        
        // Reset skill value for new question
        setSkillValue(50);
        setSkillColor('#818cf8');
        
        // In stress mode, reduce time for next question
        if (interviewConfig?.mode === 'stress') {
          setTimeLeft(prev => Math.max(30, prev - 30)); // Reduce time by 30 seconds but keep minimum 30
        }
      }, 1500); // Wait 1.5 seconds to show feedback
    } else {
      // End interview if no more questions
      handleEndInterview();
    }
  };

  const handleEndInterview = async () => {
    setIsProcessing(true);

    // Get the final transcript to send to Ollama
    const answerToSend = finalTranscript || transcript;

    try {
      // Create a comprehensive evaluation prompt for Ollama
      const evaluationPrompt = `
        Evaluate the following answer to the interview question. Provide detailed feedback in JSON format.
        
        Question: ${currentQuestion}
        Answer: ${answerToSend}
        
        Evaluate on these criteria:
        1. Overall impression (0-100)
        2. Confidence level (0-100)
        3. Communication effectiveness (0-100)
        4. Technical depth (0-100) - only if relevant to the question
        5. Relevance to question (0-100)
        
        Also identify specific strengths and areas for improvement.
        
        Respond in this exact JSON format:
        {
          "overallScore": number,
          "confidenceScore": number,
          "communicationScore": number,
          "technicalScore": number,
          "relevanceScore": number,
          "strengths": string[],
          "improvements": string[],
          "summary": string,
          "weaknessRadar": {
            "clarity": number,
            "structure": number,
            "technicalDepth": number,
            "confidence": number,
            "relevance": number
          }
        }
      `;

      const response = await fetch('http://localhost:11434/api/generate', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          model: 'llama3.2:3b',
          prompt: evaluationPrompt,
          stream: false
        })
      });

      const data = await response.json();
      
      // Extract feedback from response
      try {
        // Try to parse the response as JSON
        const jsonString = data.response.match(/\{[\s\S]*\}/)?.[0];
        if (jsonString) {
          const feedback = JSON.parse(jsonString);
          setIsProcessing(false);
          endInterview(feedback);
          return;
        }
      } catch (parseError) {
        console.error('Error parsing Ollama response:', parseError);
      }

      // If JSON parsing fails, create a basic feedback structure
      const basicFeedback = {
        overallScore: 70,
        confidenceScore: 70,
        communicationScore: 70,
        technicalScore: 70,
        relevanceScore: 70,
        strengths: ['Attempted to answer the question'],
        improvements: ['Provide more specific examples', 'Structure your response better'],
        summary: 'Your response was a good attempt. Focus on structuring your answers better and providing specific examples.',
        weaknessRadar: {
          clarity: 70,
          structure: 70,
          technicalDepth: 70,
          confidence: 70,
          relevance: 70
        }
      };

      setIsProcessing(false);
      endInterview(basicFeedback);
    } catch (error) {
      console.error('Error getting feedback from Ollama:', error);
      
      // Fallback to basic feedback if Ollama fails
      const fallbackFeedback = {
        overallScore: 65,
        confidenceScore: 65,
        communicationScore: 65,
        technicalScore: 65,
        relevanceScore: 65,
        strengths: ['Attempted to answer the question'],
        improvements: ['Provide more specific examples', 'Structure your response better'],
        summary: 'Your response was a good attempt. Focus on structuring your answers better and providing specific examples.',
        weaknessRadar: {
          clarity: 65,
          structure: 65,
          technicalDepth: 65,
          confidence: 65,
          relevance: 65
        }
      };

      setIsProcessing(false);
      endInterview(fallbackFeedback);
    }
  };

  const formatTime = (seconds: number) => {
    const mins = Math.floor(seconds / 60);
    const secs = seconds % 60;
    return `${mins}:${secs.toString().padStart(2, '0')}`;
  };

  if (isProcessing) {
    return (
      <div className="min-h-screen bg-gradient-to-br from-gray-900 via-indigo-900 to-purple-900 flex items-center justify-center">
        <div className="text-center">
          <Loader2 className="w-16 h-16 text-indigo-400 animate-spin mx-auto mb-6" />
          <h2 className="text-2xl text-white font-display font-bold mb-2">
            Analyzing your responses...
          </h2>
          <p className="text-white/60">
            Our AI is preparing your personalized feedback
          </p>
        </div>
      </div>
    );
  }

  return (
    <div className="min-h-screen bg-gradient-to-br from-gray-900 via-indigo-900 to-purple-900 pt-20 pb-8 px-4">
      <div className="max-w-4xl mx-auto">
        {/* Header */}
        <div className="flex items-center justify-between mb-6">
          <div className="flex items-center gap-4">
            <button
              onClick={() => navigateTo('dashboard')}
              className="w-10 h-10 bg-white/10 rounded-xl flex items-center justify-center hover:bg-white/20 transition-colors"
            >
              <X className="w-5 h-5 text-white" />
            </button>
            <div>
              <h1 className="text-white font-semibold">Micro Interview</h1>
              <p className="text-white/50 text-sm capitalize">{interviewConfig?.type.replace('_', ' ')} â€¢ {interviewConfig?.difficulty}</p>
            </div>
          </div>

          <div className="flex items-center gap-4">
            <div className="flex items-center gap-2 bg-white/10 px-4 py-2 rounded-full">
              <Clock className="w-4 h-4 text-white/60" />
              <span className={`text-white font-mono ${timeLeft < 60 ? 'text-red-400' : ''}`}>
                {formatTime(timeLeft)}
              </span>
            </div>
            <div className="text-white/60 text-sm">
              {currentQuestionIndex + 1} / {questions.length}
            </div>
          </div>
        </div>

        {/* Main Interview Area */}
        <div className="grid lg:grid-cols-3 gap-6">
          {/* Left - AI Avatar */}
          <div className="lg:col-span-1">
            <div className="bg-gray-800/30 backdrop-blur-sm border border-gray-700/50 rounded-2xl p-8 flex flex-col items-center">
              {/* Mode Indicator */}
              {interviewConfig?.mode && (
                <div className={`w-full mb-4 px-4 py-2 rounded-full text-center text-sm font-medium ${
                  interviewConfig.mode === 'stress' 
                    ? 'bg-red-500/20 text-red-400 border border-red-500/30' 
                    : interviewConfig.mode === 'distraction'
                    ? 'bg-yellow-500/20 text-yellow-400 border border-yellow-500/30'
                    : 'bg-blue-500/20 text-blue-400 border border-blue-500/30'
                }`}>
                  {interviewConfig.mode === 'stress' ? 'âš¡ STRESS MODE' : 
                   interviewConfig.mode === 'distraction' ? 'ðŸ”Š DISTRACTION MODE' : 'ðŸŽ¯ NORMAL MODE'}
                </div>
              )}
              
              {/* AI Avatar */}
              <div className="relative mb-6">
                <div className="ai-avatar-glow">
                  <div className={`w-32 h-32 rounded-full flex items-center justify-center shadow-lg ${
                    interviewConfig?.mode === 'stress' 
                      ? 'bg-gradient-to-br from-red-500 to-orange-500 shadow-red-500/20' 
                      : interviewConfig?.mode === 'distraction'
                      ? 'bg-gradient-to-br from-yellow-500 to-amber-500 shadow-yellow-500/20'
                      : 'bg-gradient-to-br from-indigo-500 to-purple-600 shadow-indigo-500/20'
                  }`}>
                    <span className="text-6xl">ðŸ¤–</span>
                  </div>
                </div>
                {/* Voice wave animation when speaking */}
                <div className="absolute -bottom-4 left-1/2 -translate-x-1/2 flex gap-1">
                  {[...Array(5)].map((_, i) => (
                    <div
                      key={i}
                      className={`w-1 rounded-full animate-pulse ${
                        interviewConfig?.mode === 'stress' 
                          ? 'bg-red-400' 
                          : interviewConfig?.mode === 'distraction'
                          ? 'bg-yellow-400'
                          : 'bg-indigo-400'
                      }`}
                      style={{
                        height: `${12 + Math.random() * 16}px`,
                        animationDelay: `${i * 0.1}s`
                      }}
                    />
                  ))}
                </div>
              </div>

              <h3 className="text-white font-semibold mb-1">AI Interviewer</h3>
              <p className="text-white/50 text-sm text-center">
                {interviewConfig?.mode === 'stress' 
                  ? 'Testing your performance under pressure' 
                  : interviewConfig?.mode === 'distraction'
                  ? 'Assessing your focus amid distractions'
                  : 'Listening and adapting to your responses'}
              </p>
            </div>

            {/* Live Skill Meter */}
            <div className="bg-gray-800/30 backdrop-blur-sm border border-gray-700/50 rounded-2xl p-6 mt-6">
              <h3 className="text-white font-semibold mb-4 text-center">{skillLabels[activeSkill]}</h3>
              
              {/* Gauge visualization */}
              <div className="relative h-48 flex items-center justify-center">
                {/* Circular gauge background */}
                <div className="absolute w-40 h-40 rounded-full border-4 border-gray-600"></div>
                
                {/* Gauge needle */}
                <div 
                  className="absolute w-1 h-20 origin-bottom bg-white rounded-full transition-all duration-300"
                  style={{
                    transform: `rotate(${(skillValue / 100) * 180 - 90}deg)`,
                    backgroundColor: skillColor
                  }}
                ></div>
                
                {/* Center indicator */}
                <div 
                  className="absolute w-6 h-6 rounded-full z-10"
                  style={{ backgroundColor: skillColor }}
                ></div>
                
                {/* Value indicator */}
                <div className="absolute bottom-4 text-white font-bold text-xl">
                  {Math.round(skillValue)}%
                </div>
              </div>
              
              {/* Skill hint */}
              <p className="text-gray-300 text-sm text-center mt-4">
                {skillHints[activeSkill]}
              </p>
              
              {/* Post-answer feedback */}
              {lastFeedback && (
                <div className="mt-4 p-3 bg-gray-700/50 rounded-lg border border-gray-600">
                  <p className="text-sm text-white">{lastFeedback}</p>
                </div>
              )}
            </div>
          </div>

          {/* Right - Question & Answer */}
          <div className="lg:col-span-2 space-y-6">
            {/* Question Card */}
            <div className="bg-gray-800/30 backdrop-blur-sm border border-gray-700/50 rounded-2xl p-6">
              <div className="flex items-center gap-3 mb-4">
                <div className="w-10 h-10 bg-indigo-500/20 rounded-xl flex items-center justify-center">
                  <MessageSquare className="w-5 h-5 text-indigo-400" />
                </div>
                <span className="text-white/60 text-sm">Question {currentQuestionIndex + 1}</span>
              </div>
              <p className="text-white text-lg leading-relaxed">
                {currentQuestion}
              </p>
              <button
                className="flex items-center gap-2 text-indigo-400 text-sm mt-4 hover:underline"
                onClick={() => {
                  const utterance = new SpeechSynthesisUtterance(currentQuestion);
                  utterance.rate = 0.9; // Slightly slower for better comprehension
                  utterance.pitch = 1.0;
                  utterance.volume = 0.8;
                  window.speechSynthesis.speak(utterance);
                }}
              >
                <Volume2 className="w-4 h-4" />
                Listen to question
              </button>
            </div>

            {/* Answer Area */}
            <div className="bg-gray-800/30 backdrop-blur-sm border border-gray-700/50 rounded-2xl p-6">
              <div className="flex items-center justify-between mb-4">
                <span className="text-white/60 text-sm">Your Answer</span>
                {isListening && (
                  <div className="flex items-center gap-2 text-indigo-400">
                    <div className="voice-wave">
                      {[...Array(5)].map((_, i) => (
                        <div key={i} className="voice-wave-bar" />
                      ))}
                    </div>
                    <span className="text-sm">Listening...</span>
                  </div>
                )}
              </div>

              {/* Transcript Display */}
              <div className="min-h-[120px] bg-white/5 rounded-xl p-4 mb-4">
                {transcript ? (
                  <p className="text-white">{transcript}</p>
                ) : (
                  <p className="text-white/30 italic">
                    {isListening ? 'Start speaking...' : 'Click the microphone to start answering'}
                  </p>
                )}
              </div>

              {/* Controls */}
              <div className="flex items-center justify-between">
                <div className="flex items-center gap-2">
                  <Button
                    onClick={toggleListening}
                    disabled={!isSupported}
                    className={`${isListening ? 'bg-red-500 hover:bg-red-600' : 'bg-indigo-600 hover:bg-indigo-700'} ${!isSupported ? 'opacity-50 cursor-not-allowed' : ''}`}
                  >
                    {isListening ? (
                      <>
                        <MicOff className="w-5 h-5 mr-2" />
                        Stop Recording
                      </>
                    ) : (
                      <>
                        <Mic className="w-5 h-5 mr-2" />
                        Start Answering
                      </>
                    )}
                  </Button>
                  
                  {!isSupported && (
                    <div className="flex items-center gap-2 text-red-400 text-sm">
                      <VolumeX className="w-4 h-4" />
                      <span>Speech recognition not supported</span>
                    </div>
                  )}
                </div>

                <Button
                  onClick={handleNextQuestion}
                  disabled={!(finalTranscript || transcript)}
                  className="bg-gray-700 hover:bg-gray-600"
                >
                  {currentQuestionIndex < questions.length - 1 ? 'Next Question' : 'Finish'}
                  <ArrowRight className="w-4 h-4 ml-2" />
                </Button>
              </div>
            </div>

            {/* Tips */}
            <div className="bg-blue-500/10 border border-blue-500/20 rounded-2xl p-4">
              <p className="text-blue-300 text-sm">
                <strong>Tip:</strong> Focus on the skill meter to improve your performance in real-time.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  );
}
